// @generated by protoc-gen-es v1.2.1 with parameter "target=ts"
// @generated from file aiserver/v1/inline_gpt4.proto (package aiserver.v1, syntax proto3)
/* eslint-disable */
// @ts-nocheck

import type { BinaryReadOptions, FieldList, JsonReadOptions, JsonValue, PartialMessage, PlainMessage } from "@bufbuild/protobuf";
import { Message, proto3 } from "@bufbuild/protobuf";
import { CodeBlock, CurrentFileInfo, ExplicitContext, LinterErrors, ModelDetails } from './utils_pb';
import { RepositoryInfo } from './repository_pb';

/**
 * @generated from message aiserver.v1.InlineGPT4PromptProtoV1
 */
export class InlineGPT4PromptProtoV1 extends Message<InlineGPT4PromptProtoV1> {
  /**
   * @generated from field: aiserver.v1.CurrentFileInfo current_file = 1;
   */
  currentFile?: CurrentFileInfo;

  constructor(data?: PartialMessage<InlineGPT4PromptProtoV1>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "aiserver.v1.InlineGPT4PromptProtoV1";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "current_file", kind: "message", T: CurrentFileInfo },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): InlineGPT4PromptProtoV1 {
    return new InlineGPT4PromptProtoV1().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): InlineGPT4PromptProtoV1 {
    return new InlineGPT4PromptProtoV1().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): InlineGPT4PromptProtoV1 {
    return new InlineGPT4PromptProtoV1().fromJsonString(jsonString, options);
  }

  static equals(a: InlineGPT4PromptProtoV1 | PlainMessage<InlineGPT4PromptProtoV1> | undefined, b: InlineGPT4PromptProtoV1 | PlainMessage<InlineGPT4PromptProtoV1> | undefined): boolean {
    return proto3.util.equals(InlineGPT4PromptProtoV1, a, b);
  }
}

/**
 * @generated from message aiserver.v1.StreamInlineLongCompletionRequest
 */
export class StreamInlineLongCompletionRequest extends Message<StreamInlineLongCompletionRequest> {
  /**
   * @generated from field: aiserver.v1.CurrentFileInfo current_file = 1;
   */
  currentFile?: CurrentFileInfo;

  /**
   * @generated from field: repeated aiserver.v1.RepositoryInfo repositories = 6;
   */
  repositories: RepositoryInfo[] = [];

  /**
   * @generated from field: repeated aiserver.v1.StreamInlineLongCompletionRequest.ContextBlock context_blocks = 7;
   */
  contextBlocks: StreamInlineLongCompletionRequest_ContextBlock[] = [];

  /**
   * @generated from field: aiserver.v1.ExplicitContext explicit_context = 13;
   */
  explicitContext?: ExplicitContext;

  /**
   * @generated from field: aiserver.v1.ModelDetails model_details = 14;
   */
  modelDetails?: ModelDetails;

  /**
   * @generated from field: aiserver.v1.LinterErrors linter_errors = 15;
   */
  linterErrors?: LinterErrors;

  constructor(data?: PartialMessage<StreamInlineLongCompletionRequest>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "aiserver.v1.StreamInlineLongCompletionRequest";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "current_file", kind: "message", T: CurrentFileInfo },
    { no: 6, name: "repositories", kind: "message", T: RepositoryInfo, repeated: true },
    { no: 7, name: "context_blocks", kind: "message", T: StreamInlineLongCompletionRequest_ContextBlock, repeated: true },
    { no: 13, name: "explicit_context", kind: "message", T: ExplicitContext },
    { no: 14, name: "model_details", kind: "message", T: ModelDetails },
    { no: 15, name: "linter_errors", kind: "message", T: LinterErrors },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): StreamInlineLongCompletionRequest {
    return new StreamInlineLongCompletionRequest().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): StreamInlineLongCompletionRequest {
    return new StreamInlineLongCompletionRequest().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): StreamInlineLongCompletionRequest {
    return new StreamInlineLongCompletionRequest().fromJsonString(jsonString, options);
  }

  static equals(a: StreamInlineLongCompletionRequest | PlainMessage<StreamInlineLongCompletionRequest> | undefined, b: StreamInlineLongCompletionRequest | PlainMessage<StreamInlineLongCompletionRequest> | undefined): boolean {
    return proto3.util.equals(StreamInlineLongCompletionRequest, a, b);
  }
}

/**
 * @generated from message aiserver.v1.StreamInlineLongCompletionRequest.ContextBlock
 */
export class StreamInlineLongCompletionRequest_ContextBlock extends Message<StreamInlineLongCompletionRequest_ContextBlock> {
  /**
   * @generated from field: aiserver.v1.StreamInlineLongCompletionRequest.ContextBlock.ContextType context_type = 1;
   */
  contextType = StreamInlineLongCompletionRequest_ContextBlock_ContextType.UNSPECIFIED;

  /**
   * @generated from field: repeated aiserver.v1.CodeBlock blocks = 2;
   */
  blocks: CodeBlock[] = [];

  constructor(data?: PartialMessage<StreamInlineLongCompletionRequest_ContextBlock>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "aiserver.v1.StreamInlineLongCompletionRequest.ContextBlock";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "context_type", kind: "enum", T: proto3.getEnumType(StreamInlineLongCompletionRequest_ContextBlock_ContextType) },
    { no: 2, name: "blocks", kind: "message", T: CodeBlock, repeated: true },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): StreamInlineLongCompletionRequest_ContextBlock {
    return new StreamInlineLongCompletionRequest_ContextBlock().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): StreamInlineLongCompletionRequest_ContextBlock {
    return new StreamInlineLongCompletionRequest_ContextBlock().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): StreamInlineLongCompletionRequest_ContextBlock {
    return new StreamInlineLongCompletionRequest_ContextBlock().fromJsonString(jsonString, options);
  }

  static equals(a: StreamInlineLongCompletionRequest_ContextBlock | PlainMessage<StreamInlineLongCompletionRequest_ContextBlock> | undefined, b: StreamInlineLongCompletionRequest_ContextBlock | PlainMessage<StreamInlineLongCompletionRequest_ContextBlock> | undefined): boolean {
    return proto3.util.equals(StreamInlineLongCompletionRequest_ContextBlock, a, b);
  }
}

/**
 * @generated from enum aiserver.v1.StreamInlineLongCompletionRequest.ContextBlock.ContextType
 */
export enum StreamInlineLongCompletionRequest_ContextBlock_ContextType {
  /**
   * @generated from enum value: CONTEXT_TYPE_UNSPECIFIED = 0;
   */
  UNSPECIFIED = 0,

  /**
   * @generated from enum value: CONTEXT_TYPE_RECENT_LOCATIONS = 1;
   */
  RECENT_LOCATIONS = 1,
}
// Retrieve enum metadata with: proto3.getEnumType(StreamInlineLongCompletionRequest_ContextBlock_ContextType)
proto3.util.setEnumType(StreamInlineLongCompletionRequest_ContextBlock_ContextType, "aiserver.v1.StreamInlineLongCompletionRequest.ContextBlock.ContextType", [
  { no: 0, name: "CONTEXT_TYPE_UNSPECIFIED" },
  { no: 1, name: "CONTEXT_TYPE_RECENT_LOCATIONS" },
]);

